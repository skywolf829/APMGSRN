D:\GitHub\DistributedINR\Code\..\SavedModels\Supernova_featgrid64_4layers64
Feature grid shape: [1, 16, 64, 64, 64]
Initializing dataset - reading D:\GitHub\DistributedINR\Code\Datasets\..\..\Data\Supernova.nc
Traceback (most recent call last):
  File "D:\GitHub\DistributedINR\Code\test.py", line 62, in <module>
    perform_tests(model, data, tests_to_run, opt)
  File "D:\GitHub\DistributedINR\Code\test.py", line 29, in perform_tests
    model_reconstruction(model, data, opt)
  File "D:\GitHub\DistributedINR\Code\test.py", line 17, in model_reconstruction
    result = sample_grid(model, grid, 100000)
  File "D:\GitHub\DistributedINR\Code\Models\models.py", line 49, in sample_grid
    vals = forward_maxpoints(model, coord_grid, max_points = max_points)
  File "D:\GitHub\DistributedINR\Code\Models\models.py", line 193, in forward_maxpoints
    model(coords[start:min(start+max_points, coords.shape[0])])
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\GitHub\DistributedINR\Code\Models\fVSRN.py", line 162, in forward
    y = self.decoder[i](y)
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\GitHub\DistributedINR\Code\Models\fVSRN.py", line 62, in forward
    return 0.5*x + torch.sin(x)**2
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\_tensor.py", line 39, in wrapped
    return f(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.00 GiB total capacity; 9.70 GiB already allocated; 0 bytes free; 10.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
D:\GitHub\DistributedINR\Code\..\SavedModels\Supernova_featgrid64_4layers64
Feature grid shape: [1, 16, 64, 64, 64]
Initializing dataset - reading D:\GitHub\DistributedINR\Code\Datasets\..\..\Data\Supernova.nc
Traceback (most recent call last):
  File "D:\GitHub\DistributedINR\Code\test.py", line 63, in <module>
    perform_tests(model, data, tests_to_run, opt)
  File "D:\GitHub\DistributedINR\Code\test.py", line 29, in perform_tests
    model_reconstruction(model, data, opt)
  File "D:\GitHub\DistributedINR\Code\test.py", line 17, in model_reconstruction
    result = sample_grid(model, grid, 100000)
  File "D:\GitHub\DistributedINR\Code\Models\models.py", line 49, in sample_grid
    vals = forward_maxpoints(model, coord_grid, max_points = max_points)
  File "D:\GitHub\DistributedINR\Code\Models\models.py", line 193, in forward_maxpoints
    model(coords[start:min(start+max_points, coords.shape[0])])
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\GitHub\DistributedINR\Code\Models\fVSRN.py", line 162, in forward
    y = self.decoder[i](y)
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\GitHub\DistributedINR\Code\Models\fVSRN.py", line 62, in forward
    return 0.5*x + torch.sin(x)**2
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\_tensor.py", line 39, in wrapped
    return f(*args, **kwargs)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 11.00 GiB total capacity; 9.70 GiB already allocated; 0 bytes free; 10.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
D:\GitHub\DistributedINR\Code\..\SavedModels\Supernova_featgrid64_4layers64
Feature grid shape: [1, 16, 64, 64, 64]
Initializing dataset - reading D:\GitHub\DistributedINR\Code\Datasets\..\..\Data\Supernova.nc
Traceback (most recent call last):
  File "D:\GitHub\DistributedINR\Code\test.py", line 63, in <module>
    perform_tests(model, data, tests_to_run, opt)
  File "D:\GitHub\DistributedINR\Code\test.py", line 29, in perform_tests
    model_reconstruction(model, data, opt)
  File "D:\GitHub\DistributedINR\Code\test.py", line 17, in model_reconstruction
    result = sample_grid(model, grid, 10000)
  File "D:\GitHub\DistributedINR\Code\Models\models.py", line 49, in sample_grid
    vals = forward_maxpoints(model, coord_grid, max_points = max_points)
  File "D:\GitHub\DistributedINR\Code\Models\models.py", line 193, in forward_maxpoints
    model(coords[start:min(start+max_points, coords.shape[0])])
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\GitHub\DistributedINR\Code\Models\fVSRN.py", line 162, in forward
    y = self.decoder[i](y)
  File "C:\Users\Sky\anaconda3\envs\DistINR\lib\site-packages\torch\nn\modules\module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\GitHub\DistributedINR\Code\Models\fVSRN.py", line 62, in forward
    return 0.5*x + torch.sin(x)**2
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.00 GiB total capacity; 10.24 GiB already allocated; 0 bytes free; 10.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
D:\GitHub\DistributedINR\Code\..\SavedModels\Supernova_featgrid64_4layers64
Feature grid shape: [1, 16, 64, 64, 64]
Initializing dataset - reading D:\GitHub\DistributedINR\Code\Datasets\..\..\Data\Supernova.nc
PSNR:  42.79
